---
title: "MLE in R"
author: "Mallick Hossain"
date: "2018-07-05"
slug: mle-in-r
draft: false
output:
  blogdown::html_page:
    toc: true
categories:
  - R
  - Maximum Likelihood Estimation
  - Exercise
tags: []
---



<p><strong>Note: This exercise is borrowed from <a href="https://jblevins.org/log/r-mle">Jason Blevins</a></strong></p>
<p>This is a short exercise in running maximum likelihood estimation (MLE) in R. We will do it for the following AR(1) process: <span class="math display">\[
z_t = \mu + \rho (z_{t - 1} - \mu) + \sigma \varepsilon_t
\]</span> where <span class="math inline">\(\varepsilon_t \sim N(0, 1)\)</span>.</p>
<p>Before I get into the details of MLE, what is the fundamental insight of MLE? The idea is that we want to estimate the parameters of the model that are generating our data. There are some “true” parameters that we are trying to estimate (<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\rho\)</span>, and <span class="math inline">\(\sigma\)</span>), but we cannot directly observe them. MLE asserts that for given parameters values, we can compute the probability of seeing our specific draw of the data (i.e. the “likelihood” of the data). MLE says that the best guess of these parameters is the one that maximizes this likelihood.</p>
<p>So, back to the exercise. We want to perform MLE to estimate the above AR(1) process. First, let’s get some data. We’ll assume that the true parameter values are <span class="math inline">\(\mu = -0.5\)</span>, <span class="math inline">\(\rho = 0.9\)</span>, and <span class="math inline">\(\sigma = 1\)</span>. I also set the seed of the random number generator so that anyone running this script will be able to reproduce these results.</p>
<pre class="r"><code>set.seed(100)

# True parameters
mu &lt;- -0.5
rho &lt;- 0.9
sigma &lt;- 1.0</code></pre>
<p>I’ll simulate this data for 10,000 periods and allocate a vector to hold the data.</p>
<pre class="r"><code>periods &lt;- 10000
z &lt;- double(periods)</code></pre>
<p>I need to draw an initial value of <span class="math inline">\(z_1\)</span> from which to generate the full stream of data. Plugging in <span class="math inline">\(z\)</span> recursively, we can show that the unconditional distribution of <span class="math inline">\(z\)</span> is <span class="math inline">\(N(\mu, \sigma^2 / (1 - \rho^2))\)</span></p>
<pre class="r"><code>z[1] &lt;- rnorm(1, mean = mu, sd = sigma / sqrt(1 - rho^2))</code></pre>
<p>Now, I generate the rest of the data.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<pre class="r"><code>eps &lt;- rnorm(9999)
for (i in 2:periods) {
  z[i] &lt;- mu + rho * (z[i - 1] - mu) + eps[i - 1]
}</code></pre>
<p>Let’s see what our simulated data looks like:</p>
<pre class="r"><code>plot(z, type = &quot;l&quot;)</code></pre>
<p><img src="/post/2018-07-05-mle-in-r_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>We can compare the moments with the true moments as well.</p>
<pre class="r"><code>cat(&quot;True mean: &quot;, round(mu, 2), 
    &quot;\nSample mean: &quot;, round(mean(z), 2))</code></pre>
<pre><code>## True mean:  -0.5 
## Sample mean:  -0.46</code></pre>
<pre class="r"><code>cat(&quot;True sd: &quot;, round(sigma / sqrt(1 - rho^2), 2), 
    &quot;\nSample sd: &quot;, round(sd(z), 2))</code></pre>
<pre><code>## True sd:  2.29 
## Sample sd:  2.19</code></pre>
<p>The good news is that the sample moments are close to the actual true parameters. So how do we translate this into an MLE algorithm that will output parameter estimates? This is where we need to generate our likelihood function that needs to be maximized. Basically, we need to compute the probability of the observed data given our parameter guesses. Since this is an AR(1) process, each observation is not independent, so we cannot take the product of the probabilities of observing each data point (represented below):</p>
<p><span class="math display">\[
\mathcal{L}(\theta ; z) \neq   \prod_t f(z_t | \theta)
\]</span></p>
<p>However, since our data comes from an AR(1) process, we can leverage the fact that it will be conditionally independent given the previous data point. In particular, the likelihood will be the following product:</p>
<p><span class="math display">\[
\mathcal{L}(\theta ; z) = \prod_t f(z_t | z_{t - 1}, \theta)
\]</span> To make this more convenient to work with, we take the natural log since this converts the product into a sum. <span class="math display">\[
\ell (\theta ; z) = \sum_t \ln f(z_t | z_{t - 1}, \theta)
\]</span></p>
<p>Before we can code up the likelihood, we need to know what <span class="math inline">\(f\)</span> looks like. Looking at the AR(1) representation, we can show that <span class="math inline">\(z_t | z_{t - 1} \sim N(\mu + \rho (z_{t - 1} - \mu), \sigma^2)\)</span></p>
<p>Now that we know what the conditional distribution of <span class="math inline">\(z\)</span> is, we can code our likelihood function and let R do the rest of the work. The optimizer that we will use in R will find the minimum of our function, so we take the negative of everything.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> I store our parameters in <span class="math inline">\(\theta = \{\mu, \rho, \sigma \}\)</span>.</p>
<pre class="r"><code>logl &lt;- function(theta, data) {
  mu &lt;- theta[1]
  rho &lt;- theta[2]
  sigma &lt;- theta[3]
  periods &lt;- length(data)
  sum(-dnorm(data[2:periods], 
             mean = mu + rho * (data[1:(periods - 1)] - mu), 
             sd = sigma, log = TRUE))
}</code></pre>
<p>Now we have a function which will output the log-likelihood for given values of our parameters and the data. We can use <code>nlm</code> to compute the estimated parameters (I make the initial guess 0.1 for each parameter) and store the results in <code>out</code>.</p>
<pre class="r"><code>theta1 &lt;- c(0.1, 0.1, 0.1)
out &lt;- suppressWarnings(nlm(logl, theta1, data = z))</code></pre>
<p>Let’s see how the MLE estimates compare to our true parameters.</p>
<pre class="r"><code>cat(&quot;True mu: &quot;, mu, 
    &quot;\nMLE mu: &quot;, round(out$estimate[1], 2))</code></pre>
<pre><code>## True mu:  -0.5 
## MLE mu:  -0.46</code></pre>
<pre class="r"><code>cat(&quot;True rho: &quot;, rho, 
    &quot;\nMLE rho: &quot;, round(out$estimate[2], 2))</code></pre>
<pre><code>## True rho:  0.9 
## MLE rho:  0.89</code></pre>
<pre class="r"><code>cat(&quot;True sigma: &quot;, sigma,
    &quot;\nMLE sigma: &quot;, round(out$estimate[3], 2))</code></pre>
<pre><code>## True sigma:  1 
## MLE sigma:  0.99</code></pre>
<p>Not bad! This worked out pretty well!</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For efficiency, I make all of the error draws outside of the loop. Given that we’re only doing 10,000 simulations, this probably does not significantly affect performance, but for larger simulations, it’s much better to make draws outside of a loop.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Maximizing a function is equivalent to minimizing the negative of the function<a href="#fnref2">↩</a></p></li>
</ol>
</div>
